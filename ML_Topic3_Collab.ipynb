{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KvQDvWRdSPrklTNIX1UwBNXojbhf86HJ",
      "authorship_tag": "ABX9TyM5wGPVg4lKPRySVKHh6tG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/APre203/MLTopic1/blob/main/ML_Topic3_Collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FjcGHx1Upk5-"
      },
      "outputs": [],
      "source": [
        "file_name = \"/content/drive/MyDrive/new_transcribe.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency of 100 words"
      ],
      "metadata": {
        "id": "NiWLLV7rvQZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "import string  # Import the string module to access punctuation characters\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "unXBkwlPvXP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_name, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "    clips = text.split(\"\\n\")\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "filtered_tokens = [token.lower() for token in tokens if token.isalnum()]  # Filter out punctuation and convert to lowercase\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Get the most frequent 100 words\n",
        "most_common_words = freq_dist.most_common(100) # looking for 100 most common words\n",
        "\n",
        "print(most_common_words) # prints it in a tuple (word, count) where count is the total count of the word inside the textfile\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keS2yj0aureN",
        "outputId": "092059f1-a93a-43c2-cc5d-c427b7457e37"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 73), ('of', 34), ('in', 27), ('a', 23), ('is', 18), ('and', 18), ('to', 15), ('was', 14), ('it', 11), ('he', 10), ('are', 10), ('as', 10), ('by', 9), ('also', 8), ('for', 7), ('that', 7), ('not', 6), ('at', 6), ('on', 6), ('located', 5), ('his', 5), ('were', 5), ('be', 5), ('you', 5), ('new', 4), ('this', 4), ('house', 4), ('from', 4), ('more', 3), ('or', 3), ('english', 3), ('through', 3), ('these', 3), ('colorado', 3), ('an', 3), ('population', 3), ('have', 3), ('called', 3), ('used', 3), ('according', 3), ('they', 3), ('i', 3), ('little', 2), ('note', 2), ('long', 2), ('played', 2), ('old', 2), ('has', 2), ('strategic', 2), ('increase', 2), ('music', 2), ('main', 2), ('born', 2), ('first', 2), ('river', 2), ('near', 2), ('she', 2), ('zephaniah', 2), ('number', 2), ('local', 2), ('graph', 2), ('current', 2), ('into', 2), ('known', 2), ('within', 2), ('will', 2), ('line', 2), ('settled', 2), ('however', 2), ('proved', 2), ('her', 2), ('two', 2), ('just', 2), ('mosque', 2), ('terms', 2), ('middle', 2), ('wish', 2), ('general', 2), ('when', 2), ('its', 2), ('name', 2), ('record', 2), ('appears', 2), ('add', 2), ('we', 2), ('mayor', 2), ('every', 1), ('year', 1), ('spring', 1), ('unicycle', 1), ('race', 1), ('held', 1), ('round', 1), ('tower', 1), ('all', 1), ('promise', 1), ('there', 1), ('very', 1), ('achievement', 1), ('afterward', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment of each score"
      ],
      "metadata": {
        "id": "Oma8scQzvaq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "wOV1ggwEvikE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use clips from previous section\n",
        "# Calculate sentiment scores for each clip\n",
        "sentiment_scores = []\n",
        "\n",
        "for clip in clips:\n",
        "    analysis = TextBlob(clip)\n",
        "    sentiment_scores.append(analysis.sentiment.polarity) #uses TextBlob library's built in sentiment calculation ( closer to -1 is negative sentiment and closer to 1 is positive sentiment )\n",
        "\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJMacx37veAq",
        "outputId": "db796a51-7d26-4cac-e368-43993bd83a3f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.2, -0.24375000000000002, 0.025, 0.13636363636363635, 0.1, 0.13636363636363635, 0.0, 0.0, 0.5, 0.0, 0.5, 0.08333333333333333, 0.9, 0.0, 0.0, -0.2403846153846154, 0.0, 0.0, 0.0, 0.0, 0.12878787878787878, 0.1, 0.0, 0.35714285714285715, 0.0, 0.0, 0.15000000000000002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.05, -0.1875, 0.0, 0.0, -0.05833333333333333, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.15625, 0.6, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1740909090909091, 0.0, 0.0, 0.375, 0.0, -0.06875, 0.0, 0.0, -0.1, 0.0, 0.0, 0.05000000000000002, -0.1, 0.25, -0.1, 0.0, 0.0, 0.0, 0.0, -0.1, -0.05, 0.375, 0.19999999999999998, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, -0.15833333333333333, 0.0, 0.0, 0.0, 0.16666666666666666, -0.125, 0.0, 0.0, -0.09285714285714287, 0.011111111111111118, 0.0, 0.55, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type Token Ratio"
      ],
      "metadata": {
        "id": "3j7g3CpFxWBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a list of clips in a variable called 'clips'\n",
        "num_groups = 10\n",
        "group_size = len(clips) // num_groups\n",
        "\n",
        "type_token_ratios = []\n",
        "\n",
        "for i in range(num_groups):\n",
        "    start = i * group_size\n",
        "    end = start + group_size\n",
        "    group_clips = clips[start:end]\n",
        "\n",
        "    # Calculate the type-token ratio for this group\n",
        "    tokens = nltk.word_tokenize(' '.join(group_clips))\n",
        "    unique_tokens = set(tokens)\n",
        "    ratio = len(unique_tokens) / len(tokens)\n",
        "\n",
        "    type_token_ratios.append(ratio)\n",
        "\n",
        "print(type_token_ratios)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nworXZkoxaCP",
        "outputId": "4fd6c076-153f-4a6d-fd5a-8d4a171f5b19"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7377049180327869, 0.696, 0.7009345794392523, 0.7419354838709677, 0.7391304347826086, 0.7297297297297297, 0.7622950819672131, 0.7642276422764228, 0.6625, 0.7661290322580645]\n"
          ]
        }
      ]
    }
  ]
}